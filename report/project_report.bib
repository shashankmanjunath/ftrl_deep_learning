
@article{jelassi_dual_2020,
	title = {Dual {Averaging} is {Surprisingly} {Effective} for {Deep} {Learning} {Optimization}},
	url = {http://arxiv.org/abs/2010.10502},
	abstract = {First-order stochastic optimization methods are currently the most widely used class of methods for training deep neural networks. However, the choice of the optimizer has become an ad-hoc rule that can significantly affect the performance. For instance, SGD with momentum (SGD+M) is typically used in computer vision (CV) and Adam is used for training transformer models for Natural Language Processing (NLP). Using the wrong method can lead to significant performance degradation. Inspired by the dual averaging algorithm, we propose Modernized Dual Averaging (MDA), an optimizer that is able to perform as well as SGD+M in CV and as Adam in NLP. Our method is not adaptive and is significantly simpler than Adam. We show that MDA induces a decaying uncentered \$L\_2\$-regularization compared to vanilla SGD+M and hypothesize that this may explain why it works on NLP problems where SGD+M fails.},
	urldate = {2021-10-29},
	journal = {arXiv:2010.10502 [cs, math, stat]},
	author = {Jelassi, Samy and Defazio, Aaron},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.10502},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/shashankmanjunath/Zotero/storage/NKVIARTM/Jelassi and Defazio - 2020 - Dual Averaging is Surprisingly Effective for Deep .pdf:application/pdf;arXiv.org Snapshot:/Users/shashankmanjunath/Zotero/storage/2U5IDQBC/2010.html:text/html},
}

@article{zbontar_fastmri_2019,
	title = {{fastMRI}: {An} {Open} {Dataset} and {Benchmarks} for {Accelerated} {MRI}},
	shorttitle = {{fastMRI}},
	url = {http://arxiv.org/abs/1811.08839},
	abstract = {Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.},
	urldate = {2021-10-29},
	journal = {arXiv:1811.08839 [physics, stat]},
	author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
	month = dec,
	year = {2019},
	note = {arXiv: 1811.08839},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Medical Physics, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/Users/shashankmanjunath/Zotero/storage/XHS465LR/Zbontar et al. - 2019 - fastMRI An Open Dataset and Benchmarks for Accele.pdf:application/pdf;arXiv.org Snapshot:/Users/shashankmanjunath/Zotero/storage/L8HESQME/1811.html:text/html},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@article{defazio_adaptivity_nodate,
	title = {Adaptivity without {Compromise}: {A} {Momentumized}, {Adaptive}, {Dual} {Averaged} {Gradient} {Method} for {Stochastic} {Optimization}},
	abstract = {We introduce MADGRAD, a novel optimization method in the family of AdaGrad adaptive gradient methods. MADGRAD shows excellent performance on deep learning optimization problems from multiple ﬁelds, including classiﬁcation and image-to-image tasks in vision, and recurrent and bidirectionally-masked models in natural language processing. For each of these tasks, MADGRAD matches or outperforms both SGD and ADAM in test set performance, even on problems for which adaptive methods normally perform poorly.},
	language = {en},
	author = {Defazio, Aaron and Jelassi, Samy},
	pages = {33},
	file = {Defazio and Jelassi - Adaptivity without Compromise A Momentumized, Ada.pdf:/Users/shashankmanjunath/Zotero/storage/9WW87KWS/Defazio and Jelassi - Adaptivity without Compromise A Momentumized, Ada.pdf:application/pdf},
}

@article{orabona_scale-free_2015,
	title = {Scale-{Free} {Algorithms} for {Online} {Linear} {Optimization}},
	url = {http://arxiv.org/abs/1502.05744},
	abstract = {We design algorithms for online linear optimization that have optimal regret and at the same time do not need to know any upper or lower bounds on the norm of the loss vectors. We achieve adaptiveness to norms of loss vectors by scale invariance, i.e., our algorithms make exactly the same decisions if the sequence of loss vectors is multiplied by any positive constant. Our algorithms work for any decision set, bounded or unbounded. For unbounded decisions sets, these are the first truly adaptive algorithms for online linear optimization.},
	urldate = {2021-12-01},
	journal = {arXiv:1502.05744 [cs, math]},
	author = {Orabona, Francesco and Pal, David},
	month = jul,
	year = {2015},
	note = {arXiv: 1502.05744},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/shashankmanjunath/Zotero/storage/X8BMT38S/Orabona and Pal - 2015 - Scale-Free Algorithms for Online Linear Optimizati.pdf:application/pdf;arXiv.org Snapshot:/Users/shashankmanjunath/Zotero/storage/QXL65L7G/1502.html:text/html},
}

@article{orabona_generalized_2014,
	title = {A {Generalized} {Online} {Mirror} {Descent} with {Applications} to {Classification} and {Regression}},
	url = {http://arxiv.org/abs/1304.2994},
	abstract = {Online learning algorithms are fast, memory-efficient, easy to implement, and applicable to many prediction problems, including classification, regression, and ranking. Several online algorithms were proposed in the past few decades, some based on additive updates, like the Perceptron, and some on multiplicative updates, like Winnow. A unifying perspective on the design and the analysis of online algorithms is provided by online mirror descent, a general prediction strategy from which most first-order algorithms can be obtained as special cases. We generalize online mirror descent to time-varying regularizers with generic updates. Unlike standard mirror descent, our more general formulation also captures second order algorithms, algorithms for composite losses and algorithms for adaptive filtering. Moreover, we recover, and sometimes improve, known regret bounds as special cases of our analysis using specific regularizers. Finally, we show the power of our approach by deriving a new second order algorithm with a regret bound invariant with respect to arbitrary rescalings of individual features.},
	urldate = {2021-12-01},
	journal = {arXiv:1304.2994 [cs]},
	author = {Orabona, Francesco and Crammer, Koby and Cesa-Bianchi, Nicolò},
	month = jul,
	year = {2014},
	note = {arXiv: 1304.2994},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shashankmanjunath/Zotero/storage/Z75ETD6K/Orabona et al. - 2014 - A Generalized Online Mirror Descent with Applicati.pdf:application/pdf;arXiv.org Snapshot:/Users/shashankmanjunath/Zotero/storage/D2TDB9C9/1304.html:text/html},
}

@article{duchi_adaptive_nodate,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. The adaptation, in essence, allows us to ﬁnd needles in haystacks in the form of very predictive yet rarely observed features. Our paradigm stems from recent advances in online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which signiﬁcantly simpliﬁes the task of setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We corroborate our theoretical results with experiments on a text classiﬁcation task, showing substantial improvements for classiﬁcation with sparse datasets.},
	language = {en},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	pages = {13},
	file = {Duchi et al. - Adaptive Subgradient Methods for Online Learning a.pdf:/Users/shashankmanjunath/Zotero/storage/9FKA22M3/Duchi et al. - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf},
}

@article{nesterov_quasi-monotone_2015,
	title = {Quasi-monotone {Subgradient} {Methods} for {Nonsmooth} {Convex} {Minimization}},
	volume = {165},
	issn = {0022-3239, 1573-2878},
	url = {http://link.springer.com/10.1007/s10957-014-0677-5},
	doi = {10.1007/s10957-014-0677-5},
	abstract = {In this paper, we develop new subgradient methods for solving nonsmooth convex optimization problems. These methods guarantee the best possible rate of convergence for the whole sequence of test points. Our methods are applicable as efﬁcient real-time stabilization tools for potential systems with inﬁnite horizon. Preliminary numerical experiments conﬁrm a high efﬁciency of the new schemes.},
	language = {en},
	number = {3},
	urldate = {2021-12-05},
	journal = {Journal of Optimization Theory and Applications},
	author = {Nesterov, Yu. and Shikhman, V.},
	month = jun,
	year = {2015},
	pages = {917--940},
	file = {Nesterov and Shikhman - 2015 - Quasi-monotone Subgradient Methods for Nonsmooth C.pdf:/Users/shashankmanjunath/Zotero/storage/9A7YRTPQ/Nesterov and Shikhman - 2015 - Quasi-monotone Subgradient Methods for Nonsmooth C.pdf:application/pdf},
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
	author = {Krizhevsky, Alex},
	year = {2009},
	file = {Citeseer - Snapshot:/Users/shashankmanjunath/Zotero/storage/2IYIIV7W/summary.html:text/html;Citeseer - Full Text PDF:/Users/shashankmanjunath/Zotero/storage/L62GQZBX/Krizhevsky - 2009 - Learning multiple layers of features from tiny ima.pdf:application/pdf},
}
